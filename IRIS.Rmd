---
title: "IRIS"
author: "Will"
date: "2022-11-24"
output: html_document
---


```{r}
data(iris)

## reorder/remove variables
iris_NS <- dplyr::select(iris, Sepal.Length, Sepal.Width, Petal.Length, Petal.Width)

## register the new dataset
# register("iris_NS", "iris")

```



# NON SUPERVISEE

```{r}
ggplot(data = iris_NS, aes_string(x = colnames(iris_NS)[1], y = colnames(iris_NS)[2])) +
  geom_point()
```

```{r}
plot(x = iris_NS[, colnames(iris_NS)[1]], y = iris[, colnames(iris_NS)[2]], pch = 20)
```



```{r}
seed <- 2
nbCP <- 4 # Nombre de composantes principales a representer
resPCA <- PCA(iris_NS, scale.unit = FALSE, graph = FALSE, ncp = nbCP) # scale.unit est l'argument qui permet de faire une ACP sur données réduites 
PPCAeig <- ggplot(data = cbind.data.frame(Dimension = 1:nbCP, Eigenvalue = resPCA$eig[1:nbCP, 1]), aes(x = Dimension, y = Eigenvalue)) +
  geom_bar(stat = "identity") +
  ggtitle(paste("Ebouli des", nbCP, "premières valeurs propres du groupe", seed))
PPCAind12 <- plot.PCA(resPCA, choix = "ind", label = "none", axes = c(1, 2), title = paste("Representation des individus du groupe", seed))
PPCAind34 <- plot.PCA(resPCA, choix = "ind", label = "none", axes = c(3, 4), title = paste("Representation des individus du groupe", seed))
PPCAvar <- plot.PCA(resPCA, choix = "varcor", label = "var", axes = c(1, 2), title = paste("Representation des variables du groupe", seed))

PPCAind12

```




```{r}
range(apply(iris_NS, 2, sd)) # plus petit et plus grand ecart-type
range(apply(iris_NS, 2, var)) # plus petite et plus grande variance
iris_NS.Norm <- scale(iris_NS, center = TRUE, scale = TRUE) # jeu de données centré réduit
```

## PCA et CAH

```{r}
res.PCA<-PCA(iris_NS,graph=FALSE)
plot.PCA(res.PCA,choix='var')
plot.PCA(res.PCA,title="PCA Iris_NS")

res.PCA<-PCA(iris_NS,graph=FALSE)
summary(res.PCA)

res.PCA<-PCA(iris_NS,graph=FALSE)
dimdesc(res.PCA)

res.PCA<-PCA(iris_NS,graph=FALSE)
res.HCPC<-HCPC(res.PCA,nb.clust=3,kk=100,consol=FALSE,graph=FALSE)
plot.HCPC(res.HCPC,choice='tree',title='Hierarchical tree')
plot.HCPC(res.HCPC,choice='map',draw.tree=FALSE,title='Factor map')
plot.HCPC(res.HCPC,choice='3D.map',ind.names=FALSE,centers.plot=FALSE,angle=60,title='Hierarchical tree on the factor map')


res.PCA<-PCA(iris_NS,graph=FALSE)
res.HCPC<-HCPC(res.PCA,nb.clust=3,consol=TRUE,graph=FALSE)
summary(res.HCPC)

```

## CAH DENDRO

```{r}

iris_NS.Dist <- dist(iris_NS)
iris_NS.Dendro <- hclust(iris_NS.Dist, method = "ward.D2")
plot(iris_NS.Dendro, labels = FALSE, main ="Dendrogramme")
ggdendrogram(iris_NS.Dendro, labels = FALSE)

inertie.dendro <- sort(iris_NS.Dendro$height, decreasing = TRUE)
plot(inertie.dendro[1:50], type = "s", xlab = "Nombre de classes", ylab = "Inert
ie")

plot(inertie.dendro[1:50], type = "s", xlab = "Nombre de classes", ylab = "Inert
ie") + points(c(5, 10, 15), inertie.dendro[c(5, 10, 15)], col = c("green3", "red3", "blue3"), cex = 2, lwd = 3)

Partition_CAH <- cutree(iris_NS.Dendro, k = 3)
table(Partition_CAH)

plot(rev(iris_NS.Dendro$height),type="b", xlim=c(0,10))



```


```{r}
ggplot(color_branches(iris_NS.Dendro, k = 3), labels = FALSE)
fviz_dend(iris_NS.Dendro, k = 3, show_labels = FALSE, rect = TRUE)
```

```{r}
# best.cutree(iris_NS.Dendro)

best.cutree(iris_NS.Dendro, graph = TRUE, xlab = "Nombre de classes", ylab = "Inertie relative")
```

```{r}
clustrangetest <- plot(as.clustrange(iris_NS.Dendro, iris_NS.dist))
```

![Plot title. ](IRIS_insertimage_1.png)

https://cran.r-project.org/web/packages/WeightedCluster/vignettes/WeightedClusterFR.pdf

## KMEANS

```{r}
resKmeans <- kmeans(iris_NS, centers = 3)
Partition_Kmeans <- resKmeans$cluster
table(Partition_Kmeans)
```


```{r}
intra <- rep(NA, l = 15)
intra[1] = kmeans(iris_NS, centers = 1)$tot.withinss # inertie intra k = 1
for (k in 2:15) intra[k] <- min(replicate(20, 
  kmeans(iris_NS, centers = k)$tot.withinss)) # pour chaque valeur de k (nombre de centres entre 2 et 15), on fait 20 initialisation

intra
```


```{r}
seed <- 1
set.seed(seed)
PKmeansK <- ggplot(cbind.data.frame(K = 1:15, Intra = intra), aes(K, intra)) +
  geom_point() +
  geom_path() +
  ggtitle(paste0("Inertie Intra estimée avec 15 replicats - Groupe ", seed))
PKmeansK

```


```{r}
kmeans.rep <- replicate(20, kmeans(iris_NS, centers = 3))
best <- which.min(unlist(kmeans.rep['tot.withinss', ])) # Initialisation avec la plus petite inertie intra
resKmeans <- kmeans.rep[, best]
Partition_Kmeans <- resKmeans$cluster
table(Partition_Kmeans)

iris_NS.part <- cbind.data.frame(as.factor(Partition_Kmeans), iris_NS)
resPCAPart <- PCA(iris_NS.part, scale.unit = FALSE, graph = FALSE, quali.sup = 1) # numero variable cluster
plot.PCA(resPCAPart, choix = "ind", label = "none", 
         axes = c(1, 2), 
         title = paste(
           "Representation des individus du groupe", seed),
         habillage = 1) # colorer individus categorie var 1
```

## Mélange Gaussien

```{r}
resMclust <- Mclust(iris_NS) # ATTENTION ça met beaucoup beaucoup de temps car il teste tous les modèles ...
resMclust # Affichage de l'object resultat avec le modèle choisi par mclust en fonction du critere BIC maximum (attention different du cours)
plot(resMclust)

```


```{r}
resMclust <- Mclust(iris_NS, 
                    modelNames = c("EEV", "EEE", "EVE"), 
                    G = c(1:5)) # VOIR cours pour la définition des modèles

plot(resMclust)
```


```{r}
plot(resMclust, what = c("BIC"))

```
## REVENIR DESSUS
```{r}
PPCAeig <- ggplot(data = cbind.data.frame(Dimension = 1:nbCP, Eigenvalue = resPCA$eig[1:nbCP, 1]), aes(x = Dimension, y = Eigenvalue)) +
  geom_bar(stat = "identity") +
  ggtitle(paste("Ebouli des", nbCP, "premières valeurs propres du groupe", seed))
PPCAeig # Choix du nombres d'axes dans l'ACP en observant un coude dans l'éboulis des valeurs propres par la méthode "optique"
```

```{r}
resPCAMclust <- PCA(iris_NS, scale.unit = FALSE, graph = FALSE, ncp = nbCP) # On refait tourner l'ACP pour récupérer le nombre d'axes fixés
resMclust <- Mclust(resPCAMclust$ind$coord, G = c(3:10))
```



```{r}
plot(resMclust, what = c("classification"))

Partition_Mclust <- resMclust$classification
table(Partition_Mclust)
```


```{r}
table(Partition_CAH, Partition_Kmeans)
table(Partition_CAH, Partition_Mclust)
table(Partition_Mclust, Partition_CAH, Partition_Kmeans)
```



```{r}
getProxPartition <- function(partition, resPCA){ # partition est un vecteur de classes; resPCA est le résultat de l'ACP
  classes <- unique(partition) # numeros de classes de la partition
  ProxClasses <- lapply(classes, function(classNb) (partition == classNb) %*% t(partition == classNb)) # pour chaque classe, matrice de proximité si deux individus sont ds la classe testee
  Prox <- Reduce("+", ProxClasses) # somme des matrices de proximite pour toutes les classes
  OrderInd <- order(apply(resPCA$ind$coord[, 1:2], 1, sum)) # On reordonne les individus par le 1er plan de l'ACP
  Prox <- Prox[OrderInd, OrderInd]
  return(Prox)
}

```


```{r}
ProxPartition_CAH <- getProxPartition(Partition_CAH, resPCA = resPCA)
dim(ProxPartition_CAH) # matrice 1000x1000 qui croise les individus avec 1 s'ils sont dans la même classe, 0 sinon 
image(ProxPartition_CAH)
ProxPartition_Kmeans <- getProxPartition(Partition_Kmeans, resPCA = resPCA)
image(ProxPartition_Kmeans)
ProxPartition_Mclust <- getProxPartition(Partition_Mclust, resPCA = resPCA)
image(ProxPartition_Mclust)
```


```{r}
ProxPartition <- ProxPartition_CAH + ProxPartition_Kmeans + ProxPartition_Mclust
image(ProxPartition)
```

```{r}
iris_NS.part <- cbind.data.frame(CAH = as.factor(Partition_CAH), Kmeans = as.factor(Partition_Kmeans), Mclust = as.factor(Partition_Mclust), iris_NS)
resPCAPart <- PCA(iris_NS.part, scale.unit = FALSE, graph = FALSE, quali.sup = 1:3) # scale.unit est l'argument qui permet de faire une ACP sur données réduites 
PPCAind_CAH <- plot.PCA(resPCAPart, choix = "ind", label = "none", axes = c(1, 2), title = paste("Representation des individus du groupe", seed), habillage = 1)
PPCAind_Kmeans <- plot.PCA(resPCAPart, choix = "ind", label = "none", axes = c(1, 2), title = paste("Representation des individus du groupe", seed), habillage = 2)
PPCAind_Mclust <- plot.PCA(resPCAPart, choix = "ind", label = "none", axes = c(1, 2), title = paste("Representation des individus du groupe", seed), habillage = 3)

grid.arrange(PPCAind_CAH, PPCAind_Kmeans, PPCAind_Mclust, ncol = 3, nrow = 1)

```



# SUPERVISEE

# Sous-ensemble du jeu de donnees sur lequel chaque groupe va travailler

```{r remedy025}

seed <- 2 # Remplacer par votre numero d'ordinateur (entre 41 et 62)
set.seed(seed) 

```


# ACP

```{r remedy028}

resPCA <- PCA(iris, scale.unit = TRUE, graph = FALSE, quali.sup = ncol(iris))
plot.PCA(resPCA, choix = "ind", axes = c(1, 2), label = "none", habillage = ncol(iris))

```



# Transformer en facteur la variable status

```{r remedy029}

# iris$Spicies <- as.factor(iris$Species)


```
# Tirage d?un ensemble d?apprentissage et de son ensemble de test

```{r remedy030}

set.seed(122)
IndicesTrain <- sort(sample(1:150, size = 105))
IndicesTest <- sort(setdiff(1:150, IndicesTrain))

irisTrain <- iris[IndicesTrain, ]
irisTest <- iris[IndicesTest, ]

```

# Validation croisée



```{r remedy031}

# Shuffle the dataset
Irisshuffle<-sample(nrow(iris),nrow(iris) )
# split the dataset into K groups 
K=3
K_groups<-split(Irisshuffle, ceiling(seq_along(Irisshuffle)/K))

#On cree une boucle
list_risq<-sapply(1:length(K_groups),function(i){
  test<-iris[K_groups[[i]],1:ncol(iris)-1]
  classe<-iris[-K_groups[[i]],ncol(iris)]
  test_true<-iris[K_groups[[i]],ncol(iris)]
  training<-iris[-K_groups[[i]],1:ncol(iris)-1]
  # Fit the model on the training set
  .model<-knn(train =training,test=test, cl=classe, k=5 )
  tab<-table(.model,test_true )
  #Fonction qui calcule les risques
  risque<- function(x){
    #Somme diagonale secondaire
    #diag8second<-sum(x[1:nrow(x)+(ncol(x):1-1)*nrow(x)])
     diag8second<-sum(diag(x))
    risque<-1-diag8second/sum(x)
  }
  risque(tab)
  
})
# Fonction qui prend un lambda max en paramètre et retourne un dataframe avec # les lambda et les risques de classification associés

z<-function(max_lambda){
  optim_lambda=data.frame("lambda", "risques")
  for(lambda in 1:max_lambda){
    list_risq<-sapply(1:length(K_groups),function(i){
    test<-iris[K_groups[[i]],1:ncol(iris)-1]
    classe<-iris[-K_groups[[i]],ncol(iris)]
    test_true<-iris[K_groups[[i]],ncol(iris)]
    training<-iris[-K_groups[[i]],1:ncol(iris)-1]
   # Fit the model on the training set
   .model<-knn(train =training,test=test, cl=classe, k=lambda )
    tab<-table(.model,test_true)
    #Fonction qui calcule les risques
    risque<- function(x){
    #Somme diagonale secondaire
    #diag8second<-sum(x[1:nrow(x)+(ncol(x):1-1)*nrow(x)])
     diag8second<-sum(diag(x))
     risque<-1-diag8second/sum(x)
  }
     risque(tab)
  
    })
    #print(mean(list_risq))
    optim_lambda[nrow(optim_lambda)+1,]<-c(lambda, mean(list_risq))
  }
  return (optim_lambda)
}
#Risque de classification
risque<-mean(list_risq)

#IndicesTrain<- sort(sample(1:1000, size=750))
#IndicesTest<-*
risque



# set.seed(122)
# n = 1000
# K = 10
# size = floor(n/K)
# permut = sample(1:n)
# sets = list()
# for (k in 1:K) sets[[k]]=permut[((k-1)*size+1):(k*size)]

```


# DA

# Entrainement sur donnees apprentissage

```{r remedy033}

modTrain <- MclustDA(
  data = irisTrain[, -which(colnames(irisTrain) == "Species")], #which(colnames(myP53Train != "Status"))
  class = irisTrain$Species, modelType = "EDDA", modelNames = "EEE")

summary(modTrain)

```

# Test sur les donnees test

```{r remedy034}

resMclustDA <- predict.MclustDA(modTrain, newdata = irisTest[,which(colnames(irisTest) != "Species")])

table(resMclustDA$classification) # classes predites

resMclustDA$z


```


# Pour chaque individu la somme des probabilites a posteriori fait 1

```{r remedy035}

rowSums(resMclustDA$z) # somme des elements en lignes (colSums), i.e. apply(resMclustDA$z, 1, sum)

```


# Comparaison avec les vrais labels

```{r remedy036}

Comp <- table(Vrais = irisTest$Species, Predits = resMclustDA$classification)
Risk <- (Comp[1, 2] + Comp[2, 1])/sum(Comp) # Risk <- 1 - sum(diag(Comp))/sum(Comp)

Comp
Risk

```



# Arbre de classif 

```{r remedy037}

resTree <- C5.0(
  x =  irisTrain[, -which(colnames(irisTrain) == "Species")],
  y = irisTrain$Species
)

plot(resTree)

resTree10 <- C5.0(
  x =  myP53Train[, -which(colnames(myP53Train) == "Status")],
  y = myP53Train$Status,
  control = C5.0Control(minCases = 10) 
    # smallest number of samples that must be put in at least two of the splits
)
plot(resTree10)

resRpart <- rpart(formula = Species ~ ., data = irisTrain)
plot(resRpart)
summary(resRpart)
predRpart <- predict(resRpart, newdata = irisTest) # probabilites a posteriori
predRpartLabels <- predRpart[, "active"] < 0.5
predRpartLabels <- factor(predRpartLabels)
predRpartLabels <- factor(predRpartLabels, labels = c("active", "inactive")) 
  # remplacement des labels FALSE et TRUE (ordre alphabetique) par "inactive", "active"
table(predRpartLabels)

```


# Comparaison avec les vrais labels

```{r remedy038}

Comp <- table(Vrais = irisTest$Species, Predits = predRpartLabels)
Risk <- (Comp[1, 2] + Comp[2, 1])/sum(Comp)

Risk2 <- 1 - sum(diag(Comp))/sum(Comp)

Comp
Risk

```
